{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuzann18/Loan_default/blob/main/Applied_Data_Science_Loan_Default_Zuzanna_Walus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_iHEvciuTB9"
      },
      "source": [
        "# **Loan Default Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKyzwpUiuTB2"
      },
      "source": [
        "\n",
        "## **Problem Definition**\n",
        "\n",
        "### **The Context: Why is this Problem Important?**\n",
        "\n",
        "Loan default is a major challenge in retail banking, directly affecting profitability and risk exposure. Manual, judgment-based credit assessment processes are prone to inefficiency and bias. Automating this process using data-driven methods reduces human error, ensures regulatory compliance (e.g., Equal Credit Opportunity Act), and provides fairer, more transparent lending decisions.\n",
        "Additional business view:\n",
        "Beyond loss mitigation, advanced risk prediction enables better portfolio management, dynamic product pricing, and enhanced customer experience, all of which contribute to competitive advantage.\n",
        "\n",
        "#### **Additional Business Perspective**\n",
        "From a strategic point of view, automating and optimizing credit risk evaluation brings:\n",
        "- **Operational efficiency** – reducing processing time and manpower requirements.\n",
        "- **Improved risk management** – avoiding bad loans improves balance sheet health.\n",
        "- **Regulatory alignment** – adhering to frameworks like the Equal Credit Opportunity Act ensures fair lending practices.\n",
        "- **Competitive advantage** – faster and more accurate decisions enhance customer experience and loyalty.\n",
        "\n",
        "### **The Objectives: What is the Goal?**\n",
        "\n",
        "The primary goal is to build a binary classification model that predicts whether a loan applicant is likely to default (BAD = 1) or repay (BAD = 0), using the features available at the time of loan application.\n",
        "\n",
        "#### **Extended Objective View**\n",
        "- Proactively **identify high-risk applicants** before loan disbursal.\n",
        "- Offer **interpretable and explainable** decisions to justify loan denials.\n",
        "- Generate **feature-based recommendations** for refining the bank's approval policy.\n",
        "- Aid in **bias mitigation** by ensuring that historical discrimination does not propagate through the model.\n",
        "\n",
        "### **The Key Questions**\n",
        "\n",
        "- Which variables are most predictive of loan default?\n",
        "- What is the best performing model (accuracy vs interpretability) for this problem?\n",
        "- How can the model be validated for fairness and generalizability?\n",
        "- What steps should the bank take based on model insights?\n",
        "- Can we balance business objectives (low false negatives) with regulatory fairness?\n",
        "\n",
        "### **The Problem Formulation: Data Science View**\n",
        "\n",
        "This problem is a supervised binary classification task. Given historical data on loan applications and their outcomes, we aim to:\n",
        "\n",
        "- Train a model to predict the `BAD` label based on input features like income, employment status, existing debts, property value, and more.\n",
        "- **Focus on predicting `BAD = 1` (loan default), since the business impact of lending to defaulters is high.**\n",
        "- **Minimize False Negatives (i.e., cases where a defaulter is incorrectly classified as non-defaulter), which represent the costliest errors.**\n",
        "- Evaluate models using classification metrics (e.g., ROC-AUC, Recall, Precision-Recall Curve, F1-Score) appropriate for imbalanced datasets.\n",
        "- Emphasize model interpretability (e.g., using logistic regression, decision trees, SHAP) to support business deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEHRGpcdo-KO"
      },
      "source": [
        "## **Data Description:**\n",
        "The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable that indicates whether an applicant has ultimately defaulted or has been severely delinquent. This adverse outcome occurred in 1,189 cases (20 percent). 12 input variables were registered for each applicant.\n",
        "\n",
        "\n",
        "* **BAD:** 1 = Client defaulted on loan, 0 = loan repaid\n",
        "\n",
        "* **LOAN:** Amount of loan approved.\n",
        "\n",
        "* **MORTDUE:** Amount due on the existing mortgage.\n",
        "\n",
        "* **VALUE:** Current value of the property.\n",
        "\n",
        "* **REASON:** Reason for the loan request. (HomeImp = home improvement, DebtCon= debt consolidation which means taking out a new loan to pay off other liabilities and consumer debts)\n",
        "\n",
        "* **JOB:** The type of job that loan applicant has such as manager, self, etc.\n",
        "\n",
        "* **YOJ:** Years at present job.\n",
        "\n",
        "* **DEROG:** Number of major derogatory reports (which indicates a serious delinquency or late payments).\n",
        "\n",
        "* **DELINQ:** Number of delinquent credit lines (a line of credit becomes delinquent when a borrower does not make the minimum required payments 30 to 60 days past the day on which the payments were due).\n",
        "\n",
        "* **CLAGE:** Age of the oldest credit line in months.\n",
        "\n",
        "* **NINQ:** Number of recent credit inquiries.\n",
        "\n",
        "* **CLNO:** Number of existing credit lines.\n",
        "\n",
        "* **DEBTINC:** Debt-to-income ratio (all your monthly debt payments divided by your gross monthly income. This number is one way lenders measure your ability to manage the monthly payments to repay the money you plan to borrow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZcGaZruTB-"
      },
      "source": [
        "## **Import the necessary libraries and Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1OWEEoP9RStK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7501f477-232a-40a0-fd81-e83429daf787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pingouin\n",
            "  Downloading pingouin-0.5.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pingouin) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pingouin) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from pingouin) (2.2.2)\n",
            "Collecting pandas-flavor (from pingouin)\n",
            "  Downloading pandas_flavor-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from pingouin) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pingouin) (1.15.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from pingouin) (0.13.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from pingouin) (0.14.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from pingouin) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->pingouin) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->pingouin) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->pingouin) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->pingouin) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->pingouin) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pingouin) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pingouin) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pingouin) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pingouin) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pingouin) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pingouin) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pingouin) (3.2.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from pandas-flavor->pingouin) (2025.3.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->pingouin) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5->pingouin) (1.17.0)\n",
            "Downloading pingouin-0.5.5-py3-none-any.whl (204 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.4/204.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_flavor-0.7.0-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pandas-flavor, pingouin\n",
            "Successfully installed pandas-flavor-0.7.0 pingouin-0.5.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pingouin\n",
        "\n",
        "# Import necessary libraries\n",
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        ")\n",
        "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "import scipy.stats as st\n",
        "from mlxtend.evaluate import mcnemar_table, mcnemar, cochrans_q\n",
        "import pingouin as pg\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import joblib\n",
        "import shap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvj9AGJtuTB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "975a1ece-8735-4aa3-aad9-fd2c1d43a118"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65d17bcf-a6af-45b3-8048-5a4e45371c73\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-65d17bcf-a6af-45b3-8048-5a4e45371c73\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load dataset\n",
        "#df = pd.read_csv('hmeq.csv')\n",
        "# Get the actual filename from the uploaded dictionary\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLgLExFVBZtg"
      },
      "source": [
        "\n",
        "# Data overview: shape, types, nulls, duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lJFjmt_5csh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('hmeq.csv')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za7znZ1cBZtg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data overview: shape, types, nulls, duplicates\n",
        "print(\"Shape of dataset:\", df.shape)\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oWVJPaN5csi"
      },
      "outputs": [],
      "source": [
        "# Select numeric and categorical columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Separate count-type vs continuous numeric features\n",
        "count_like_vars = []\n",
        "continuous_vars = []\n",
        "\n",
        "for col in numeric_cols:\n",
        "    unique_vals = df[col].dropna().unique()\n",
        "    if pd.api.types.is_integer_dtype(df[col]) and df[col].nunique() < 20:\n",
        "        count_like_vars.append(col)\n",
        "    elif col != 'BAD':  # Exclude target\n",
        "        continuous_vars.append(col)\n",
        "\n",
        "# Define target variable\n",
        "target_var = 'BAD' if 'BAD' in df.columns else None\n",
        "\n",
        "# Output results\n",
        "print(\"Detected Feature Types:\")\n",
        "print(f\"- Continuous numeric: {continuous_vars}\")\n",
        "print(f\"- Count (discrete integers): {count_like_vars}\")\n",
        "print(f\"- Categorical: {categorical_cols}\")\n",
        "if target_var:\n",
        "    print(f\"- Target variable: {target_var}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om37iBxRRStN"
      },
      "outputs": [],
      "source": [
        "print(\"\\nMissing Values Check:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "print(\"\\nNumber of Duplicate Rows:\")\n",
        "print(df.duplicated().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0jl9_gSVMCL"
      },
      "outputs": [],
      "source": [
        "def format_number(x):\n",
        "    \"\"\"\n",
        "    Format a numeric value to 3 decimal places, but if the value is zero (or nearly zero)\n",
        "    then return '0' (as string). Also remove unnecessary trailing zeros.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        num = float(x)\n",
        "        # If the absolute value is negligibly small, print 0\n",
        "        if abs(num) < 1e-7:\n",
        "            return \"0\"\n",
        "        # Format with three decimals first\n",
        "        s = f\"{num:.3f}\"\n",
        "        # Remove trailing zeros and a trailing dot if any\n",
        "        s = s.rstrip('0').rstrip('.') if '.' in s else s\n",
        "        return s\n",
        "    except Exception:\n",
        "        return x\n",
        "\n",
        "# Example: Calculate and format the missing_percentage series from the notebook\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "formatted_missing_percentage = missing_percentage.apply(format_number)\n",
        "print(\"Missing percentage per column %:\")\n",
        "print(formatted_missing_percentage)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng1cyEUqRStO"
      },
      "outputs": [],
      "source": [
        "missing_like = ['NA', 'N/A', 'na', 'n/a', 'null', 'NULL', '', ' ', None]\n",
        "\n",
        "for col in df.columns:\n",
        "    missing_count = df[col].isin(missing_like).sum()\n",
        "    if missing_count > 0:\n",
        "        print(f\"{col} has {missing_count} suspicious missing-like values.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aPM34A6RStO"
      },
      "outputs": [],
      "source": [
        "for col in df.select_dtypes(include='object'):\n",
        "    stripped = df[col].astype(str).str.strip()\n",
        "    print(f\"{col}: {(stripped == '').sum()} empty strings after strip.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhLPNVpPRStP"
      },
      "outputs": [],
      "source": [
        "print(\"\\nUnique Values per Column:\")\n",
        "print(df.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHTODkjLuTCT"
      },
      "source": [
        "## Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hRbpzF0BZth"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Summary statistics\n",
        "summary = df.describe().round(3)\n",
        "summary = summary.applymap(\n",
        "    lambda x: f\"{x:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
        "    if isinstance(x, float) else x\n",
        ")\n",
        "\n",
        "print(\"\\nStatistical Summary:\\n\", summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCJFfL3DRStQ"
      },
      "source": [
        "## Observations Summary Statistics\n",
        "\n",
        "**Correlation Insights:**\n",
        "- **DELINQ** (delinquent credit lines) shows a moderate positive correlation (0.354) with **BAD** (loan default), indicating a higher number of delinquencies may be associated with defaults.\n",
        "- **DEROG** (major derogatory reports) correlates positively (0.276) with **BAD**, suggesting its potential as a risk indicator.\n",
        "- Other features like **LOAN**, **MORTDUE**, and **VALUE** are strongly inter-correlated, which may imply multicollinearity issues that need attention during modeling.\n",
        "\n",
        "**Data Quality and Missing Values:**\n",
        "- The main DataFrame has complete records for loan performance and applicant details, but the *outliers* subset reveals missing values in fields like **DEBTINC** and some categorical variables.\n",
        "- Missing observations in variables such as **MORTDUE**, **VALUE**, **JOB**, etc., are evident in the *outliers* DataFrame, which might affect model training if not properly imputed.\n",
        "\n",
        "**Outlier Detection:**\n",
        "- The *outliers* DataFrame contains 1201 records, emphasizing the need for careful treatment of extreme values, especially in features like **DELINQ**, **DEROG**, and **DEBTINC**.\n",
        "\n",
        "**General Observations:**\n",
        "- The dataset contains 5960 records with 13 attributes, combining both numerical and categorical data.\n",
        "- Attention should be paid to handling multicollinearity and the imputation of missing values to ensure robust model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKYAgeeXRStQ"
      },
      "source": [
        "## Treating Missing  - Imputation\n",
        "\n",
        "- In this section  \n",
        "1) IterativeImputer (MICE)\n",
        "2) Random sampling\n",
        "\n",
        "### Enable the experimental IterativeImputer (MICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2eiOckMRStR"
      },
      "outputs": [],
      "source": [
        "# Enable the experimental IterativeImputer (MICE)\n",
        "# Identify numeric columns\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Apply iterative imputation (MICE) to numeric features\n",
        "mice_imputer = IterativeImputer(random_state=42)\n",
        "df[num_cols] = mice_imputer.fit_transform(df[num_cols])\n",
        "\n",
        "# Check that missing values have been imputed\n",
        "print(\"Missing values after MICE imputation:\")\n",
        "print(df[num_cols].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM6vX_Pa5csn"
      },
      "source": [
        "### Handling Missing Values for Nominal Unordered Variables (JOB, REASON) - Random Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1T4ylIL5csn"
      },
      "source": [
        "Statistical operations based on ordering (such as mean or median) are inappropriate and meaningless for such data types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbS3mATm5csn"
      },
      "source": [
        "#### Indexes of Nans for REASON before imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mt3-7jhbV-S"
      },
      "outputs": [],
      "source": [
        "nan_list = df[df['REASON'].isna()].index.tolist()\n",
        "print(nan_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePibJdHH5csn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='REASON', data=df)\n",
        "plt.title('Distribution of REASON before Randomized Imputation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qYYK41c5cso"
      },
      "source": [
        "#### Random Sampling REASON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb0uAzUVbK0N"
      },
      "outputs": [],
      "source": [
        "probs = df['REASON'].value_counts(normalize=True)\n",
        "mask_nan = df['REASON'].isna()\n",
        "n_nan = mask_nan.sum()\n",
        "draws = np.random.choice(probs.index, size=n_nan, p=probs.values)\n",
        "df.loc[mask_nan, 'REASON'] = draws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S4L0Ghc5cso"
      },
      "source": [
        "This method:\n",
        "\n",
        "1)Preserves **original proportions** between DebtCon and HomeImp.\n",
        "\n",
        "2)**Avoids introducing bias** towards a particular category.\n",
        "\n",
        "3)Maintains data realism, important for model performance and interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xIQrAVc5cso"
      },
      "source": [
        "#### Nans of REASON after imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TviNOl_bZaw"
      },
      "outputs": [],
      "source": [
        "nan_list = df[df['REASON'].isna()].index.tolist()\n",
        "print(nan_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6RTL8Bk5csp"
      },
      "outputs": [],
      "source": [
        "# Plotting REASON distribution after imputation\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='REASON', data=df)\n",
        "plt.title('Distribution of REASON after Randomized Imputation')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i02uUb645csp"
      },
      "outputs": [],
      "source": [
        "# Calculate the probability distribution of existing non-missing JOB values\n",
        "probs = df['JOB'].value_counts(normalize=True)\n",
        "\n",
        "# Identify missing values\n",
        "mask_nan = df['JOB'].isna()\n",
        "n_nan = mask_nan.sum()\n",
        "\n",
        "# Randomly draw replacements based on the observed distribution\n",
        "draws = np.random.choice(probs.index, size=n_nan, p=probs.values)\n",
        "\n",
        "# Fill missing values with the drawn random categories\n",
        "df.loc[mask_nan, 'JOB'] = draws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW6vPSKR5csp"
      },
      "source": [
        "#### Why Use Random Sampling Instead of Mode?\n",
        "Mode-only imputation could overpopulate the most common category (e.g., \"Other\"), artificially simplifying the job distribution.\n",
        "\n",
        "Random sampling preserves the natural variability of occupations.\n",
        "\n",
        "Especially important in credit scoring: different job types may have different risk profiles (e.g., self-employed vs salaried employees)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08llp04e5csp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='JOB', data=df)\n",
        "plt.title('Distribution of JOB after Randomized Imputation')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHDT6GH7RStR"
      },
      "source": [
        "## **Exploratory Data Analysis (EDA) and Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZS3CBPGRStR"
      },
      "source": [
        "### **Univariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6nw5BWbRStS"
      },
      "source": [
        "#### Checking Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnUcCtx55csq"
      },
      "outputs": [],
      "source": [
        "def plot_continuous_outliers(df, column):\n",
        "    \"\"\"\n",
        "    Plots a histogram with KDE and a boxplot for a continuous numeric column,\n",
        "    using IQR-based outlier thresholds. It also prints the count and percentage\n",
        "    of outliers below and above the bounds (with a '%' sign).\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The input DataFrame.\n",
        "        column (str): The name of the continuous numeric column to analyze.\n",
        "    \"\"\"\n",
        "    data = df[column].dropna()\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Plot histogram with KDE and add vertical lines for lower and upper bounds.\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
        "    sns.histplot(data, bins=20, kde=True, color='skyblue', ax=axes[0])\n",
        "    axes[0].axvline(lower_bound, color='red', linestyle='--', label='Lower Bound')\n",
        "    axes[0].axvline(upper_bound, color='green', linestyle='--', label='Upper Bound')\n",
        "    axes[0].set_title(f\"Histogram and KDE of {column}\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot boxplot with outlier boundaries\n",
        "    sns.boxplot(x=data, color='steelblue', ax=axes[1])\n",
        "    axes[1].axvline(lower_bound, color='red', linestyle='--')\n",
        "    axes[1].axvline(upper_bound, color='green', linestyle='--')\n",
        "    axes[1].set_title(f\"Boxplot of {column}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    total_count = len(data)\n",
        "    outliers_below = (data < lower_bound).sum()\n",
        "    outliers_above = (data > upper_bound).sum()\n",
        "    percentage_below = (outliers_below / total_count) * 100\n",
        "    percentage_above = (outliers_above / total_count) * 100\n",
        "\n",
        "    print(f\"\\n{column} IQR Outlier Boundaries:\")\n",
        "    print(f\" - Lower Bound: {lower_bound:.2f}\")\n",
        "    print(f\" - Upper Bound: {upper_bound:.2f}\")\n",
        "    print(f\" - Outliers below lower bound: {outliers_below} ({percentage_below:.2f}%)\")\n",
        "    print(f\" - Outliers above upper bound: {outliers_above} ({percentage_above:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itlN4Jhg5csv"
      },
      "source": [
        "### Boxplot: DEROG Distribution by Loan Default (BAD)\n",
        "\n",
        "This boxplot compares the distribution of `DEROG` (major derogatory credit reports) between non-defaulters (`BAD = 0`) and defaulters (`BAD = 1`).\n",
        "\n",
        "**Key observations:**\n",
        "- Applicants who defaulted (`BAD = 1`) tend to have significantly higher values of `DEROG`, with many outliers above 2 and some reaching as high as 10.\n",
        "- In contrast, most non-defaulters (`BAD = 0`) have zero or near-zero `DEROG` values.\n",
        "- The difference in distribution between the two groups reinforces the importance of `DEROG` as a risk factor for loan default prediction.\n",
        "\n",
        "This insight supports the inclusion of `DEROG` in the final model and may also justify a binning or transformation strategy to improve performance in linear models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUVr4bYe5csw"
      },
      "outputs": [],
      "source": [
        "# Compare DEROG distribution between default status groups\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x='BAD', y='DEROG', data=df, palette='Set2')\n",
        "plt.title(\"DEROG Distribution by Loan Default (BAD)\")\n",
        "plt.xlabel(\"Loan Default (BAD: 0 = No, 1 = Yes)\")\n",
        "plt.ylabel(\"DEROG\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7r3mdP85csw"
      },
      "source": [
        "### Feature Analysis: `DEROG` – Major Derogatory Reports\n",
        "\n",
        "**Description:**  \n",
        "The `DEROG` variable captures the number of **major derogatory credit reports** associated with an applicant. These reports typically reflect **severe negative credit events**, such as bankruptcies, foreclosures, or charge-offs. As such, `DEROG` serves as a strong proxy for historical financial delinquency and is expected to be highly predictive of loan default behavior.\n",
        "\n",
        "**Distribution Characteristics:**  \n",
        "- The distribution of `DEROG` is **right-skewed**, with most values concentrated at `0`, indicating no major derogatory records.\n",
        "- A small portion of applicants exhibit high `DEROG` values, making them natural **outliers**.\n",
        "- These outliers are **informative rather than erroneous** and should be preserved, particularly for tree-based models which are robust to such skewness.\n",
        "\n",
        "**Correlation with Target:**  \n",
        "- `DEROG` shows a **moderate positive correlation** with the target variable `BAD` (loan default), suggesting that applicants with more derogatory reports are more likely to default.\n",
        "\n",
        "**Treatment Strategy:**  \n",
        "- For models like **Random Forest** and **XGBoost**, `DEROG` is used **without transformation**, as these models can natively handle skewed numeric features.\n",
        "- For **logistic regression** or other linear models, it may be beneficial to:\n",
        "  - Apply a **log-transform** (e.g., `log1p(DEROG)`) to reduce skewness, or\n",
        "  - Use **binning** to convert the continuous variable into categorical risk bands (e.g., `0`, `1`, `2–3`, `>3`).\n",
        "\n",
        "**Conclusion:**  \n",
        "The `DEROG` variable is a **business-critical indicator of creditworthiness** and is retained in its original form for tree-based models. It plays a central role in identifying high-risk applicants and is one of the top-ranked features according to SHAP values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xerFnGRStT",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### Detailed Variable Overview and Outlier Analysis\n",
        "\n",
        "- **LOAN (Loan Amount):**  \n",
        "    - **Description:** Represents the approved loan amount.  \n",
        "    - **Outlier Considerations:** The range is wide, with several extreme high values visible in the boxplot. These outliers may represent unusually large loans that can influence mean-based statistics. Analyzing the histogram reveals right-skewness.\n",
        "\n",
        "- **MORTDUE (Mortgage Due):**  \n",
        "    - **Description:** The outstanding amount due on an existing mortgage.  \n",
        "    - **Outlier Considerations:** Missing values are present and extreme values can be spotted in the boxplot. High values indicate potential risk exposures and may require transformation or capping.\n",
        "\n",
        "- **VALUE (Property Value):**  \n",
        "    - **Description:** The current market value of the property used as collateral.  \n",
        "    - **Outlier Considerations:** Exhibits significant variability with some very high values. Outliers in this variable can distort analyses and may need logarithmic transformation.\n",
        "\n",
        "- **REASON (Loan Request Purpose):**  \n",
        "    - **Description:** Categorical variable denoting whether the loan is for home improvement (HomeImp) or debt consolidation (DebtCon).  \n",
        "    - **Outlier Considerations:** Outlier analysis is less applicable here; however, attention is required regarding rare or missing categories.\n",
        "\n",
        "- **JOB (Applicant’s Job Type):**  \n",
        "    - **Description:** Categorical description of the applicant’s employment (e.g., Manager, Office, Other).  \n",
        "    - **Outlier Considerations:** Similar to REASON, outliers in categorical variables are not defined by extreme numeric values, but missing or rarely occurring classes should be treated carefully.\n",
        "\n",
        "- **YOJ (Years at Job):**  \n",
        "    - **Description:** Number of years the applicant has been at their current job.  \n",
        "    - **Outlier Considerations:** The boxplot may reveal cases with exceptionally long or short job tenures which could be due to data entry issues or genuine outlying behavior.\n",
        "\n",
        "- **DEROG (Number of Derogatory Reports):**  \n",
        "    - **Description:** Indicates the count of major derogatory events on the credit report.  \n",
        "    - **Outlier Considerations:** A relatively low count is expected; however, a few applicants with multiple reports appear as outliers in the boxplot, signaling severe past credit problems.\n",
        "\n",
        "- **DELINQ (Number of Delinquent Credit Lines):**  \n",
        "    - **Description:** Represents the number of past delinquent credit lines.  \n",
        "    - **Outlier Considerations:** Outlier analysis using the IQR method shows that while most applicants have 0 or 1 record, some have unusually higher numbers which could significantly affect risk modeling.\n",
        "\n",
        "- **CLAGE (Age of Oldest Credit Line):**  \n",
        "    - **Description:** The age (in months) of the oldest active credit line.  \n",
        "    - **Outlier Considerations:** Outliers with very high CLAGE values indicate long credit histories. Such extremes are visible in the boxplot and may suggest special credit patterns.\n",
        "\n",
        "- **NINQ (Number of Recent Inquiries):**  \n",
        "    - **Description:** The total number of recent credit inquiries.  \n",
        "    - **Outlier Considerations:** The majority cluster at low values; however, several higher values are evident in the histogram, which might impact the distribution’s tail.\n",
        "\n",
        "- **CLNO (Number of Credit Lines):**  \n",
        "    - **Description:** Total count of credit lines the applicant currently holds.  \n",
        "    - **Outlier Considerations:** Outliers here could represent applicants with an unusually high number of credit accounts, which might be a sign of overextension.\n",
        "\n",
        "- **DEBTINC (Debt-to-Income Ratio):**  \n",
        "    - **Description:** Ratio summarizing the debt payments relative to income.  \n",
        "    - **Outlier Considerations:** The distribution is often skewed, with some extreme values in the boxplot. These high ratios are critical risk indicators and may dominate the untransformed data’s variability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWNWhTcl5csx"
      },
      "source": [
        "### Cap values in 'DEBTINC' at the calculated lower and upper bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1abvZVGLRStU"
      },
      "outputs": [],
      "source": [
        "# Calculate lower and upper bounds for 'DEBTINC' using the IQR method\n",
        "Q1 = df['DEBTINC'].quantile(0.25)\n",
        "Q3 = df['DEBTINC'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Cap values in 'DEBTINC' at the calculated lower and upper bounds\n",
        "df['DEBTINC'] = df['DEBTINC'].apply(lambda x: lower_bound if x < lower_bound else (upper_bound if x > upper_bound else x))\n",
        "\n",
        "# Verify that outliers have been treated\n",
        "num_below = (df['DEBTINC'] < lower_bound).sum()\n",
        "num_above = (df['DEBTINC'] > upper_bound).sum()\n",
        "print(\"Outliers after treatment: Number of values below lower_bound:\", num_below)\n",
        "print(\"Outliers after treatment: Number of values above upper_bound:\", num_above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9lf-oNxRStU",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### Univariate Analysis Observations\n",
        "\n",
        "1. **Continuous Monetary Variables:**\n",
        "    - Variables such as LOAN, MORTDUE, and VALUE exhibit right-skewed distributions, with most observations concentrated at the lower end and a long tail indicating few very high amounts.\n",
        "    - The spread in these distributions suggests the presence of significant variability in loan sizes and property values.\n",
        "\n",
        "2. **Tenure and Age Variables:**\n",
        "    - YOJ (Years at Present Job) and CLAGE (Age of the Oldest Credit Line) show distributions that are closer to normal but still indicate some skewness.\n",
        "    - These patterns imply that while most applicants have moderate job tenure and credit history, there are outliers with unusually high or low values.\n",
        "\n",
        "3. **Downgrade and Delinquency Measures:**\n",
        "    - Count metrics like DEROG (Number of Derogatory Reports) and DELINQ (Number of Delinquent Credit Lines) are highly concentrated at low counts; most applicants have zero or one report.\n",
        "    - The distribution of NINQ (Number of Recent Credit Inquiries) similarly indicates that a majority of applicants have very few inquiries.\n",
        "\n",
        "4. **Implications for Modeling:**\n",
        "    - The right-skewed nature of monetary variables might benefit from transformations (e.g., log transformation) to reduce the influence of extreme values.\n",
        "    - The concentrated counts in delinquency measures validate their use as strong indicators for distinguishing between low- and high-risk applicants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg5IFtbouTCa"
      },
      "source": [
        "### **Bivariate Analysis relationship with target 'BAD'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b60NSTMBZtj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Bivariate analysis: relationship with target 'BAD'\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='BAD', y='LOAN', data=df)\n",
        "plt.title('Loan Amount by Default Status')\n",
        "plt.show()\n",
        "\n",
        "# Cross-tab for REASON\n",
        "pd.crosstab(df['REASON'], df['BAD'], normalize='index') * 100\n",
        "\n",
        "#  Loan amount comparison between defaulters and non-defaulters\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x='BAD', y='LOAN', data=df)\n",
        "plt.title(\"Loan Amount Comparison: Defaulters vs Non-Defaulters\")\n",
        "plt.xlabel(\"Defaulted (BAD)\")\n",
        "plt.ylabel(\"Loan Amount\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#  Mortgage amount comparison between defaulters and non-defaulters\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x='BAD', y='MORTDUE', data=df)\n",
        "plt.title(\"Mortgage Amount Comparison: Defaulters vs Non-Defaulters\")\n",
        "plt.xlabel(\"Defaulted (BAD)\")\n",
        "plt.ylabel(\"Mortgage Amount\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMnHgwjrRStV",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### Observations from the bivariate analysis on loan amount:\n",
        "\n",
        "1. The boxplot comparing loan amounts against default status (BAD) shows noticeable differences in distribution between defaulters and non-defaulters.\n",
        "2. **Non-defaulters generally have a higher median loan amount and a wider range**, suggesting that larger loans are more common among borrowers who do not default.\n",
        "3. Defaulters tend to have comparatively lower loan amounts; however, there is substantial overlap in the distributions, indicating that **loan amount alone does not fully explain default behavior**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E-nzkZyRStV"
      },
      "outputs": [],
      "source": [
        "# Select key features for panel analysis\n",
        "panel_features = ['DEBTINC', 'CLAGE', 'DELINQ', 'MORTDUE', 'LOAN', 'VALUE', 'BAD']\n",
        "\n",
        "# Subset DataFrame for pairplot\n",
        "pairplot_df = df[panel_features].copy()\n",
        "pairplot_df = pairplot_df.dropna()  # drop NAs for clean plotting\n",
        "\n",
        "# Plot pairplot with 'BAD' as hue (target)\n",
        "sns.set(style=\"ticks\", font_scale=1.1)\n",
        "g = sns.pairplot(\n",
        "    pairplot_df,\n",
        "    hue='BAD',\n",
        "    palette={0: \"tab:blue\", 1: \"tab:orange\"},\n",
        "    diag_kind=\"kde\",\n",
        "    plot_kws={'alpha': 0.5, 's': 15},\n",
        "    corner=True\n",
        ")\n",
        "g.fig.suptitle(\"pairplot with 'BAD'\", y=1.02, fontsize=18)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xHDWRIMRStW",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### Observations Based on Panel Distributions\n",
        "\n",
        "- **Distinct Clustering by Default Status:**  \n",
        "    The pairplot reveals clear separation between defaulters (BAD = 1) and non‑defaulters (BAD = 0) in key features. While some features like DEBTINC and DELINQ show stronger separation, others overlap more, hinting at complex interdependencies.\n",
        "\n",
        "- **Variability in Financial Measures:**  \n",
        "    Features such as LOAN, MORTDUE, and VALUE exhibit wide variances across the applicant pool. Non‑defaulters tend to have a broader spread in loan and property values compared to defaulters, suggesting that high loan amounts are generally associated with safer profiles.\n",
        "\n",
        "- **Credit Behavior Indicators:**  \n",
        "    Variables like CLAGE (age of the oldest credit line) and DELINQ (number of delinquent credit lines) display noticeable differences between the two groups. Higher values in these features are associated with an increased likelihood of default, reflecting historical credit challenges.\n",
        "\n",
        "- **Consistent Patterns Across Features:**  \n",
        "    The panel distribution confirms that while each feature provides individual insight, their combined visualization presents a richer narrative. Certain attributes, when observed in tandem (for example, high debt-to-income ratio along with multiple delinquencies), highlight clusters of high risk.\n",
        "\n",
        "- **Usefulness for Feature Engineering:**  \n",
        "    The observed distribution patterns guide targeted feature transformations and selection. For instance, applying logarithmic transformations to skewed monetary variables or carefully treating outliers could improve model robustness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc9wZJcGuTCm"
      },
      "source": [
        "### **Multivariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FgBdui_5csz"
      },
      "outputs": [],
      "source": [
        "# Correlation heatmap with significance indicators\n",
        "\n",
        "# Subset numeric columns from df to avoid converting non-numeric data\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "# Compute correlation matrix from the numeric columns\n",
        "corr = numeric_df.corr()\n",
        "\n",
        "# Compute p-values matrix using Pearson correlation on numeric data\n",
        "pvals = pd.DataFrame(np.ones(corr.shape), columns=corr.columns, index=corr.index)\n",
        "for row in corr.index:\n",
        "    for col in corr.columns:\n",
        "        if row == col:\n",
        "            pvals.loc[row, col] = 0.0  # Perfect correlation\n",
        "        else:\n",
        "            _, p = st.pearsonr(numeric_df[row], numeric_df[col])\n",
        "            pvals.loc[row, col] = p\n",
        "\n",
        "# Create annotations: append \"*\" if p < 0.05\n",
        "annot_corr = corr.copy()\n",
        "for i in corr.index:\n",
        "    for j in corr.columns:\n",
        "        p_value = pvals.loc[i, j]\n",
        "        star = \"*\" if p_value < 0.05 else \"\"\n",
        "        annot_corr.loc[i, j] = f\"{corr.loc[i, j]:.2f}{star}\"\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, annot=annot_corr, cmap=\"coolwarm\", fmt=\"\", square=True)\n",
        "plt.title(\"Correlation Heatmap with Significance Indicators (p < 0.05)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB3JkXS5RStX",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### Observations from the Correlation Heatmap\n",
        "\n",
        "- **Strong Intercorrelations:**  \n",
        "    The heatmap shows that monetary variables such as **LOAN**, **MORTDUE**, and **VALUE** are highly correlated. This suggests that multicollinearity may be present, which should be considered during feature engineering and model interpretation.\n",
        "\n",
        "- **Risk Indicator Relationships:**  \n",
        "    Features related to credit behavior, such as **DELINQ** and **DEROG**, display moderate correlations with the target variable (**BAD**), reinforcing their potential importance as risk indicators.\n",
        "\n",
        "- **Numerical Variables Spread:**  \n",
        "    Other numerical features like **YOJ**, **CLAGE**, and **DEBTINC** have lower correlation values with the target but may still contribute to the model in subtle ways. Their relationships should be further explored to understand complex interactions.\n",
        "\n",
        "- **Implications for Modeling:**  \n",
        "    The observed correlations highlight the need for careful preprocessing. Techniques like feature selection or regularization might be useful to mitigate the effects of multicollinearity while retaining the predictive power of the key features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmW5KCozRStf"
      },
      "outputs": [],
      "source": [
        "# 1. Range of values for the loan amount variable \"LOAN\"\n",
        "loan_min = df['LOAN'].min()\n",
        "loan_max = df['LOAN'].max()\n",
        "print(f\"Range of LOAN: Min = {loan_min}, Max = {loan_max}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B75miqiwRStf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3. Number of unique categories in the REASON variable\n",
        "unique_reasons = df['REASON'].nunique()\n",
        "print(f\"Number of unique categories in REASON: {unique_reasons}\")\n",
        "\n",
        "# 4. Most common category in the JOB variable\n",
        "most_common_job = df['JOB'].mode()[0]\n",
        "print(f\"Most common category in JOB: {most_common_job}\")\n",
        "\n",
        "\n",
        "# 7. Correlation between property value and loan default rate\n",
        "correlation_property_bad = df[['VALUE', 'BAD']].corr().iloc[0, 1]\n",
        "print(f\"Correlation between property value and loan default rate: {correlation_property_bad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-AD_dOaRStg"
      },
      "source": [
        "###  Default Rates by Loan Purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiPNnHrvRStg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 5. Relationship between REASON and proportion of applicants who defaulted\n",
        "reason_default_rate = df.groupby('REASON')['BAD'].mean()\n",
        "print(\"\\nProportion of applicants who defaulted by REASON:\")\n",
        "print(reason_default_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpCfwyIWRStg",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "\n",
        "\n",
        "The analysis shows that the default rate for applicants applying for Debt Consolidation (DebtCon) is approximately 18.97%, whereas for Home Improvement (HomeImp) it is about 22.25%. This indicates that loans requested for Home Improvement carry a slightly higher risk of default compared to those for Debt Consolidation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NxkLXpTRSth"
      },
      "source": [
        "### Observations\n",
        "\n",
        "1. **Correlation with Target Variable (`BAD`)**:\n",
        "   - The target variable `BAD` shows moderate positive correlations with delinquency-related variables:\n",
        "     - `DELINQ` has the highest positive correlation (0.354) with `BAD`, suggesting that a higher number of delinquent credit lines is associated with a higher risk of default.\n",
        "     - `DEROG` also shows a positive correlation (0.276) with `BAD`, indicating that major derogatory reports are linked to default risk.\n",
        "\n",
        "2. **Numerical and Categorical Variables**:\n",
        "   - The loan amount (`LOAN`) varies widely, ranging from **1,100** to **89,900**, which could imply segmentation in loan sizes.\n",
        "   - The most common category in the `JOB` variable is **\"Other\"**, indicating that many applicants might fall into a general or less defined job category.\n",
        "\n",
        "3. **Default Rates by Loan Purpose (`REASON`)**:\n",
        "   - The `REASON` variable, which indicates the loan request reason (`DebtCon` and `HomeImp`), has different default rates:\n",
        "     - Approximately **18.97%** for `DebtCon`.\n",
        "     - Approximately **22.25%** for `HomeImp`.\n",
        "   - This insight could help in tailoring risk models based on the purpose of the loan.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG5o8HdQRSth"
      },
      "source": [
        "### Class Imbalance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZICrlgNRSth"
      },
      "outputs": [],
      "source": [
        "# Check class distribution of target variable 'BAD'\n",
        "\n",
        "\n",
        "# Count values\n",
        "class_counts = df['BAD'].value_counts()\n",
        "class_percent = df['BAD'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Display class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(class_counts)\n",
        "print(\"\\nPercentage Distribution:\")\n",
        "print(class_percent)\n",
        "\n",
        "# Plot class distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='BAD', data=df, palette='viridis')\n",
        "plt.title('Target Variable Distribution: BAD (Loan Default)')\n",
        "plt.xlabel('BAD (0 = No Default, 1 = Default)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDQrKjeRSth"
      },
      "source": [
        "### Treating imbalance\n",
        "\n",
        "The dataset shows a class imbalance, with approximately 80% of the cases labeled as `0` (no default) and only 20% as `1` (default). This imbalance could lead to biased model performance favoring the majority class.\n",
        "\n",
        "To address this, we apply **SMOTE (Synthetic Minority Oversampling Technique)** to balance the classes in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdqhtr8yuS-L"
      },
      "source": [
        "## **Model Building - Approach**\n",
        "- Data preparation\n",
        "- Partition the data into train and test set\n",
        "- Build the model\n",
        "- Fit on the train data\n",
        "- Tune the model\n",
        "- Test the model on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfYYGXaRSti"
      },
      "source": [
        "## **Data Preprocessing**\n",
        "\n",
        "- Imputation for missing values was performed\n",
        "- In this section :\n",
        "1) Label encoding for categorical features.\n",
        "2) Scaling of numerical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEjfAMeqRSti"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Drop rows with missing target\n",
        "df = df[df['BAD'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X43EpBkZRSti"
      },
      "outputs": [],
      "source": [
        "# Splitting data into features and target\n",
        "X = df.drop('BAD', axis=1)\n",
        "y = df['BAD']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FPpLUDrRSti"
      },
      "source": [
        "\n",
        "# Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQjiujLqRSti"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsgtJsI_RStj"
      },
      "source": [
        "### Justification for Test Size Selection\n",
        "\n",
        "Choosing the right `test_size` is a trade-off, especially in imbalanced classification problems where SMOTE is applied only to the training set:\n",
        "\n",
        "- A **test_size of 0.3 (30%)** allows for more examples of the minority class (`BAD = 1`) to remain in the test set. This is critical, because although SMOTE balances the training set, the model is evaluated on the real-world, imbalanced distribution.\n",
        "- Having a sufficient number of minority-class examples in the test set ensures reliable estimates of metrics such as **recall**, **F1-score**, and **ROC-AUC**, which are particularly sensitive to class imbalance.\n",
        "- A **test_size of 0.2 (20%)**, while allocating more data to training, can make the evaluation unreliable due to too few positive (default) samples in the test set.\n",
        "\n",
        "**Recommendation:** With ~6000 rows and only ~20% default cases, using `test_size=0.3` ensures a meaningful and balanced evaluation phase, without overly compromising training size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0v09xFARStj"
      },
      "source": [
        "## Feature Preprocessing with ColumnTransformer\n",
        "\n",
        "To prepare the features for modeling, a column-wise preprocessing pipeline is constructed using `ColumnTransformer`.  \n",
        "- **Numerical features** are standardized to zero mean and unit variance using `StandardScaler`, which often improves convergence and model interpretability.\n",
        "- **Categorical features** are one-hot encoded (with the first category dropped to avoid multicollinearity), allowing models to treat categorical values as separate, independent features.\n",
        "\n",
        "This modular pipeline ensures that all subsequent modeling steps use the same, correctly preprocessed feature space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QsfkgbcRStk"
      },
      "outputs": [],
      "source": [
        "\n",
        "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)\n",
        "])\n",
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "X_test_proc = preprocessor.fit_transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGUxkmPXRStk"
      },
      "source": [
        "\n",
        "## **Base Modeling: Logistic Regression, Random Forest, and XGBoost**\n",
        "\n",
        "We train and evaluate three classification models:\n",
        "- Logistic Regression (interpretable baseline).\n",
        "- Random Forest (ensemble of decision trees).\n",
        "- XGBoost (gradient boosting technique).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AgldbdaRStk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_proc, y_train)\n",
        "lr_preds = lr.predict(X_test_proc)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_proc, y_train)\n",
        "rf_preds = rf.predict(X_test_proc)\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb.fit(X_train_proc, y_train)\n",
        "xgb_preds = xgb.predict(X_test_proc)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Logistic Regression:\")\n",
        "print(classification_report(y_test, lr_preds))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, lr.predict_proba(X_test_proc)[:, 1]))\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(classification_report(y_test, rf_preds))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test_proc)[:, 1]))\n",
        "\n",
        "print(\"XGBoost:\")\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, xgb.predict_proba(X_test_proc)[:, 1]))\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, ConfusionMatrixDisplay\n",
        "\n",
        "# Logistic Regression\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr.predict_proba(X_test_proc)[:, 1])\n",
        "lr_auc = auc(lr_fpr, lr_tpr)\n",
        "\n",
        "# Random Forest\n",
        "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf.predict_proba(X_test_proc)[:, 1])\n",
        "rf_auc = auc(rf_fpr, rf_tpr)\n",
        "\n",
        "# XGBoost\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb.predict_proba(X_test_proc)[:, 1])\n",
        "xgb_auc = auc(xgb_fpr, xgb_tpr)\n",
        "\n",
        "# Plot ROC Curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lr_fpr, lr_tpr, label=f\"Logistic Regression (AUC = {lr_auc:.2f})\")\n",
        "plt.plot(rf_fpr, rf_tpr, label=f\"Random Forest (AUC = {rf_auc:.2f})\")\n",
        "plt.plot(xgb_fpr, xgb_tpr, label=f\"XGBoost (AUC = {xgb_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "plt.title(\"ROC Curves\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrices\n",
        "models = {'Logistic Regression': lr, 'Random Forest': rf, 'XGBoost': xgb}\n",
        "for name, model in models.items():\n",
        "    print(f\"Confusion Matrix for {name}:\")\n",
        "    ConfusionMatrixDisplay.from_estimator(model, X_test_proc, y_test, cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix: {name}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71vv3iuTRStk",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### **Evaluation of Model Performance with Visualizations**\n",
        "\n",
        "- **Logistic Regression:**\n",
        "    - **Accuracy**: 0.84\n",
        "    - **Macro-average F1-score**: 0.68\n",
        "    - **ROC-AUC**: 0.7624\n",
        "    - **Observations from Visualizations**:\n",
        "        - The ROC curve for Logistic Regression shows a moderate ability to distinguish between defaulters and non-defaulters, with an AUC of 0.76.\n",
        "        - The confusion matrix indicates that while the model performs well for the majority class (non-defaulters), it struggles to correctly classify defaulters (class 1), leading to lower recall for this class.\n",
        "    - **Summary**: Logistic Regression provides a good baseline but lacks the ability to effectively identify defaulters, as seen in the ROC curve and confusion matrix.\n",
        "\n",
        "- **Random Forest:**\n",
        "    - **Accuracy**: 0.91\n",
        "    - **F1-score for default class (1.0)**: 0.75\n",
        "    - **ROC-AUC**: 0.9592\n",
        "    - **Observations from Visualizations**:\n",
        "        - The ROC curve for Random Forest is steep and close to the top-left corner, indicating excellent discrimination between defaulters and non-defaulters, with an AUC of 0.96.\n",
        "        - The confusion matrix shows a significant improvement in correctly classifying defaulters compared to Logistic Regression, with fewer false negatives.\n",
        "    - **Summary**: Random Forest demonstrates strong performance, effectively balancing precision and recall for both classes, as reflected in the ROC curve and confusion matrix.\n",
        "\n",
        "- **XGBoost:**\n",
        "    - **ROC-AUC**: 0.9528\n",
        "    - **Observations from Visualizations**:\n",
        "        - The ROC curve for XGBoost is very similar to that of Random Forest, with an AUC of 0.95, indicating competitive performance.\n",
        "        - The confusion matrix highlights that XGBoost also performs well in identifying defaulters, with a slightly higher number of false negatives compared to Random Forest.\n",
        "    - **Summary**: XGBoost is a robust model with performance comparable to Random Forest. Its slightly lower recall for defaulters compared to Random Forest is reflected in the confusion matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### **Overall Summary**\n",
        "- **ROC Curves**: The ROC curves clearly show that ensemble methods (Random Forest and XGBoost) outperform Logistic Regression in terms of discrimination power, with Random Forest achieving the highest AUC.\n",
        "- **Confusion Matrices**: Both Random Forest and XGBoost significantly reduce false negatives compared to Logistic Regression, making them more effective for identifying defaulters.\n",
        "- Random Forest is the best-performing model based on ROC-AUC, confusion matrix insights, and overall classification metrics. It is well-suited for this loan default prediction task, offering a balance between accuracy and interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHHnvHf_RStl"
      },
      "source": [
        "## Model Training and Evaluation Framework\n",
        "\n",
        "### Modular Experiment Management\n",
        "\n",
        "All experiments are managed using the custom `ExperimentManager` class, which provides a modular and reproducible framework for machine learning model development. This class supports:\n",
        "\n",
        "- Model registration with consistent preprocessing and optional SMOTE oversampling for class imbalance.\n",
        "- Automated hyperparameter tuning using cross-validated grid search.\n",
        "- Unified model evaluation and reporting of key classification metrics.\n",
        "- Advanced statistical comparison between models.\n",
        "\n",
        "### Handling Class Imbalance\n",
        "\n",
        "Each model is configured with `class_weight='balanced'`, ensuring that the learning algorithm compensates for class imbalance during training. This parameter automatically adjusts the weights inversely proportional to class frequencies in the data, making misclassifications of the minority class more costly for the model. As a result, the model is less biased towards the majority class and is better able to recognize patterns from the minority (target) class, which is critical for imbalanced classification tasks.\n",
        "\n",
        "### Model Pipelines\n",
        "\n",
        "The following models are included in the evaluation, all with `class_weight='balanced'`:\n",
        "\n",
        "- **XGBoost**\n",
        "- **Random Forest**\n",
        "- **Logistic Regression**\n",
        "\n",
        "Each model is integrated into a pipeline with:\n",
        "- Common data preprocessing steps,\n",
        "- Optional SMOTE (Synthetic Minority Over-sampling Technique) for further class balancing,\n",
        "- Grid search over relevant hyperparameters.\n",
        "\n",
        "### Experimental Workflow\n",
        "\n",
        "1. **Model Initialization**  \n",
        "   Models are initialized with `class_weight='balanced'`, ensuring fair treatment of both classes during training.\n",
        "\n",
        "2. **Model Training and Hyperparameter Tuning**  \n",
        "   Each model undergoes hyperparameter tuning using stratified cross-validation and the macro F1-score as the optimization metric.\n",
        "\n",
        "3. **Evaluation**  \n",
        "   The best estimators are evaluated on a separate test set. Performance is measured using accuracy, precision, recall, F1-score, and ROC AUC.\n",
        "\n",
        "4. **Cross-Validation and Statistical Comparison**  \n",
        "   Cross-validated results are visualized and statistically compared (Wilcoxon, t-test, Kruskal-Wallis, McNemar, Cochran’s Q, ANOVA) to assess the significance of differences between models.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The combined use of `class_weight='balanced'` and SMOTE ensures robust handling of class imbalance both at the data level (SMOTE) and the algorithmic level (`class_weight='balanced'`). This setup provides a fair evaluation of each model’s true discriminative power, especially for the minority class, which is of primary interest in this project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XZM0SNlRStl"
      },
      "outputs": [],
      "source": [
        "class ExperimentManager:\n",
        "    \"\"\"\n",
        "    Modular experiment manager for ML model pipelines.\n",
        "    Handles model registration, hyperparameter tuning, evaluation,\n",
        "    cross-validation, and statistical comparison.\n",
        "    \"\"\"\n",
        "    def __init__(self, preprocessor, cv_splits=5, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize manager with a preprocessor, cross-validation setup, and random state.\n",
        "        \"\"\"\n",
        "        self.preprocessor = preprocessor\n",
        "        self.cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
        "        self.random_state = random_state\n",
        "        self.models = {}\n",
        "        self.best_estimators_ = {}\n",
        "        self.best_params_ = {}\n",
        "        self.best_scores_ = {}\n",
        "\n",
        "    def add_model(self, name, estimator, param_grid=None, use_smote=False):\n",
        "        \"\"\"\n",
        "        Add a new model pipeline, with optional SMOTE and hyperparameter grid.\n",
        "\n",
        "        Parameters:\n",
        "            name (str): Name of the model.\n",
        "            estimator: Estimator/classifier instance.\n",
        "            param_grid (dict): Grid for hyperparameter tuning.\n",
        "            use_smote (bool): Whether to apply SMOTE.\n",
        "        \"\"\"\n",
        "        steps = [('preprocessor', self.preprocessor)]\n",
        "        if use_smote:\n",
        "            steps.append(('smote', SMOTE(random_state=self.random_state)))\n",
        "        steps.append(('clf', estimator))\n",
        "        pipeline = ImbPipeline(steps) if use_smote else SklearnPipeline(steps)\n",
        "        self.models[name] = {'pipeline': pipeline, 'param_grid': param_grid}\n",
        "\n",
        "    def tune(self, X, y, scoring='roc_auc', use_cv=True):\n",
        "        \"\"\"\n",
        "        Tune models using GridSearchCV or train directly if no param_grid.\n",
        "        Stores the best estimators for downstream evaluation.\n",
        "\n",
        "        Parameters:\n",
        "            X: Training features.\n",
        "            y: Training labels.\n",
        "            scoring (str): Scoring metric for GridSearchCV.\n",
        "            use_cv (bool): Whether to use cross-validation for hyperparameter tuning.\n",
        "        \"\"\"\n",
        "        # Clear previous results if any\n",
        "        self.best_estimators_ = {}\n",
        "        self.best_params_ = {}\n",
        "        self.best_scores_ = {}\n",
        "        for name, cfg in self.models.items():\n",
        "            pipe = cfg['pipeline']\n",
        "            if use_cv and cfg['param_grid']:\n",
        "                gs = GridSearchCV(pipe, cfg['param_grid'], cv=self.cv,\n",
        "                                  scoring=scoring, n_jobs=-1)\n",
        "                gs.fit(X, y)\n",
        "                self.best_estimators_[name] = gs.best_estimator_\n",
        "                self.best_params_[name] = gs.best_params_\n",
        "                self.best_scores_[name] = gs.best_score_\n",
        "                print(f\"{name} best {scoring}: {gs.best_score_:.4f}\")\n",
        "            else:\n",
        "                pipe.fit(X, y)\n",
        "                self.best_estimators_[name] = pipe\n",
        "                self.best_params_[name] = None\n",
        "                self.best_scores_[name] = None\n",
        "                print(f\"{name} trained without CV\")\n",
        "                pipe.fit(X, y)\n",
        "                self.best_estimators_[name] = pipe\n",
        "                print(f\"{name} trained without CV\")\n",
        "\n",
        "    def evaluate(self, X_train, X_test_proc, y_train, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate best estimators on holdout data.\n",
        "        Returns DataFrame with accuracy, precision, recall, f1, and roc_auc.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for name, est in self.best_estimators_.items():\n",
        "            est.fit(X_train, y_train)\n",
        "            y_pred = est.predict(X_test_proc)\n",
        "            y_prob = est.predict_proba(X_test_proc)[:,1] if hasattr(est, 'predict_proba') else None\n",
        "            results.append({\n",
        "                'model': name,\n",
        "                'accuracy': accuracy_score(y_test, y_pred),\n",
        "                'precision': precision_score(y_test, y_pred),\n",
        "                'recall': recall_score(y_test, y_pred),\n",
        "                'f1': f1_score(y_test, y_pred),\n",
        "                'roc_auc': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
        "            })\n",
        "        return pd.DataFrame(results).set_index('model')\n",
        "\n",
        "    def cross_val_scores(self, X, y, scoring):\n",
        "        \"\"\"\n",
        "        Compute cross-validation scores for each model.\n",
        "        Returns dict: model name -> scores for each CV fold.\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        for name, cfg in self.models.items():\n",
        "            scorer = make_scorer(scoring) if callable(scoring) else scoring\n",
        "            cv_scores = cross_val_score(\n",
        "                cfg['pipeline'], X, y, cv=self.cv, scoring=scorer, n_jobs=-1\n",
        "            )\n",
        "            scores[name] = cv_scores\n",
        "        return scores\n",
        "\n",
        "# --- Model registration ---\n",
        "mgr = ExperimentManager(preprocessor)\n",
        "\n",
        "\"\"\" Register Xgboost, Random Forest, and Logistic Regression models\n",
        "    with respective hyperparameter grids and SMOTE balancing. \"\"\"\n",
        "mgr.add_model(\n",
        "    'XGBoost',\n",
        "    XGBClassifier(\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        tree_method='hist',\n",
        "        scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum()  # Handle imbalance\n",
        "    ),\n",
        "    param_grid={\n",
        "        'clf__n_estimators': [100, 200],\n",
        "        'clf__max_depth': [3, 6, 10],\n",
        "        'clf__learning_rate': [0.01, 0.1, 0.3],\n",
        "        'clf__subsample': [0.8, 1.0],\n",
        "        'clf__colsample_bytree': [0.8, 1.0],\n",
        "        'clf__reg_alpha': [0, 0.5],\n",
        "        'clf__reg_lambda': [1, 2]\n",
        "    },\n",
        "    use_smote=True\n",
        ")\n",
        "mgr.add_model(\n",
        "    'RandomForest',\n",
        " RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
        "    param_grid={'clf__n_estimators': [100, 200],\n",
        "                'clf__max_depth': [10, 20],\n",
        "                'clf__min_samples_split': [2, 5],\n",
        "                'clf__min_samples_leaf': [1, 2],\n",
        "                'clf__bootstrap': [True, False],\n",
        "                'clf__criterion': ['gini', 'entropy']},\n",
        "    use_smote=True\n",
        ")\n",
        "mgr.add_model(\n",
        "    'LogisticRegression',\n",
        "    LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
        "    param_grid={\n",
        "    'clf__penalty': ['l2'],\n",
        "    'clf__C': [0.1, 1, 10],\n",
        "    'clf__solver': ['liblinear', 'lbfgs'],\n",
        "    'clf__max_iter': [500]},\n",
        "    use_smote=True\n",
        ")\n",
        "\n",
        "\"\"\" Perform hyperparameter tuning using cross-validation (GridSearchCV). \"\"\"\n",
        "mgr.tune(X_train, y_train, scoring='f1_macro', use_cv=True)\n",
        "\n",
        "\n",
        "print(\"Best hyperparameters and CV scores for each model:\\n\")\n",
        "for name in mgr.models:\n",
        "    if mgr.best_params_[name] is not None:\n",
        "        print(f\"{name}:\")\n",
        "        print(\"  Best hyperparameters:\", mgr.best_params_[name])\n",
        "        print(f\"  Best CV score: {mgr.best_scores_[name]:.4f}\\n\")\n",
        "    else:\n",
        "        print(f\"{name}: No hyperparameter tuning was performed.\\n\")\n",
        "\n",
        "\"\"\" Evaluate final models on test set and report main classification metrics. \"\"\"\n",
        "results_df = mgr.evaluate(X_train, X_test, y_train, y_test)\n",
        "print(\"\\nPerformance after tuning - GridSearch:\\n\", results_df)\n",
        "\n",
        "\"\"\" Cross-validation: compute F1 (macro) scores for each model across all folds. \"\"\"\n",
        "cv_scores = mgr.cross_val_scores(\n",
        "    X_train, y_train,\n",
        "    scoring=lambda yt, yp: f1_score(yt, yp, average='macro')\n",
        ")\n",
        "\n",
        "\"\"\" Visualize CV score distributions for each model. \"\"\"\n",
        "plt.figure(); sns.boxplot(data=list(cv_scores.values()));\n",
        "plt.xticks(range(len(cv_scores)), list(cv_scores.keys()));\n",
        "plt.title('F1 (macro) across CV folds'); plt.show()\n",
        "\n",
        "# --- Statistical comparison ---\n",
        "\"\"\" Compare models statistically: Wilcoxon, t-test, Kruskal-Wallis. \"\"\"\n",
        "### Define helper function for p-value formatting\n",
        "def p_fmt(p):\n",
        "    s = f\"{p:.3f}\"\n",
        "    return \"<0.0001\" if s == \"0.000\" else s\n",
        "\n",
        "\"\"\" Compare models statistically: Wilcoxon, t-test, Kruskal-Wallis. \"\"\"\n",
        "names = list(cv_scores.keys())\n",
        "stat_w, p_w = st.wilcoxon(cv_scores[names[0]], cv_scores[names[1]])\n",
        "stat_t, p_t = st.ttest_ind(cv_scores[names[0]], cv_scores[names[1]])\n",
        "print(f\"\\nWilcoxon {names[0]} vs {names[1]}: stat={stat_w:.3f}, p={p_fmt(p_w)}\")\n",
        "print(f\"T-test {names[0]} vs {names[1]}: stat={stat_t:.3f}, p={p_fmt(p_t)}\")\n",
        "stat_k, p_k = st.kruskal(*cv_scores.values())\n",
        "print(f\"Kruskal-Wallis: stat={stat_k:.3f}, p={p_fmt(p_k)}\")\n",
        "\n",
        "\"\"\" Predict on test set and compute McNemar and Cochran's Q for predictions. \"\"\"\n",
        "preds = {}\n",
        "for name, est in mgr.best_estimators_.items():\n",
        "    est.fit(X_train, y_train)\n",
        "    preds[name] = est.predict(X_test)\n",
        "\n",
        "tbl = mcnemar_table(y_test, preds[names[0]], preds[names[1]])\n",
        "chi2_m, p_m = mcnemar(tbl)\n",
        "print(f\"\\nMcNemar {names[0]} vs {names[1]}:\\n\", tbl)\n",
        "print(f\"chi2={chi2_m:.3f}, p={p_m:.3f}\")\n",
        "\n",
        "cq_stat, cq = cochrans_q(y_test.to_numpy(), *[preds[n] for n in names])\n",
        "print(f\"\\nCochran's Q: {cq:.3f}\")\n",
        "\n",
        "\"\"\" ANOVA-based comparison for CV scores (Welch ANOVA + Games-Howell post-hoc). \"\"\"\n",
        "t_scores = np.hstack([cv_scores[n] for n in names])\n",
        "models_col = sum([[n]*len(cv_scores[n]) for n in names], [])\n",
        "df_scores = pd.DataFrame({'cv_score': t_scores, 'model': models_col})\n",
        "anova = pg.welch_anova(data=df_scores, dv='cv_score', between='model')\n",
        "posthoc = pg.pairwise_gameshowell(data=df_scores, dv='cv_score', between='model')\n",
        "print(\"\\nWelch ANOVA:\\n\", anova)\n",
        "print(\"\\nPost-hoc Games-Howell:\\n\", posthoc)\n",
        "\n",
        "# Save the best model\n",
        "best_model_name = max(mgr.best_scores_, key=mgr.best_scores_.get)\n",
        "best_model = mgr.best_estimators_[best_model_name]\n",
        "joblib.dump(best_model, 'best_model.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kRKbIlTRStl"
      },
      "source": [
        "\n",
        "## Statistical Comparison of Model Performance\n",
        "\n",
        "To ensure the observed differences between models are meaningful and not due to random variation, several statistical tests were performed. Here is an interpretation of the results:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Pairwise Tests Between XGBoost and Random Forest**\n",
        "\n",
        "- **Wilcoxon Test**:  \n",
        "  - Statistic = 3.000, p = 0.312  \n",
        "  - *Interpretation*: The Wilcoxon signed-rank test checks for significant differences in the paired cross-validation scores between XGBoost and Random Forest.  \n",
        "  - **Conclusion:** p > 0.05, so we do **not** find a statistically significant difference in median performance between these models.\n",
        "\n",
        "- **T-Test**:  \n",
        "  - Statistic = 0.487, p = 0.639  \n",
        "  - *Interpretation*: The independent t-test compares the mean scores across cross-validation folds.\n",
        "  - **Conclusion:** Again, p > 0.05 means no statistically significant difference in means.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Kruskal-Wallis Test**\n",
        "\n",
        "- **Statistic = 9.620, p = 0.008**  \n",
        "- *Interpretation*: This non-parametric test checks whether at least one model out of the group has a distribution of scores different from the others.\n",
        "- **Conclusion:** p < 0.05 indicates a statistically significant difference exists **somewhere** among the models, prompting post-hoc pairwise comparisons.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **McNemar’s Test (XGBoost vs Random Forest)**\n",
        "\n",
        "- **Contingency Table:**  \n",
        "  ```\n",
        "  [[1584   66]\n",
        "   [  30  108]]\n",
        "  ```\n",
        "- **chi2 = 12.76, p = 0**\n",
        "- *Interpretation*: McNemar’s test assesses the differences in classification errors between two models on the same test set.\n",
        "- **Conclusion:** p < 0.05 shows there is a significant difference in the pattern of misclassifications.  \n",
        "  This suggests that while overall metrics (like mean F1) may be similar, the models make errors on different samples.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Cochran’s Q Test**\n",
        "\n",
        "- **Statistic = 0**\n",
        "- *Interpretation*: This tests whether there are differences in performance among multiple classifiers across the same dataset.\n",
        "- **Conclusion:** Here, the value indicates no significant difference detected—possibly due to the specific configuration or high agreement among predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Welch ANOVA & Post-hoc Games-Howell**\n",
        "\n",
        "- **Welch ANOVA**:  \n",
        "  - F = 105.11, p = 0.000002  \n",
        "  - *Interpretation*: Welch’s ANOVA is robust to unequal variances and tests whether mean performance differs among models.\n",
        "  - **Conclusion:** Very low p-value confirms a statistically significant difference between at least some model means.\n",
        "\n",
        "- **Post-hoc Games-Howell**:  \n",
        "  - Compares all pairs, accounting for variance differences.  \n",
        "  - *Key results*:  \n",
        "    - Logistic Regression is significantly outperformed by both Random Forest and XGBoost (very low p-values).\n",
        "    - XGBoost and Random Forest: mean difference is small and **not statistically significant** (p = 0.88).\n",
        "  - **Conclusion:** The main difference is between the ensemble models and Logistic Regression; between XGBoost and Random Forest, differences are minimal.\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary of Statistical Insights**\n",
        "\n",
        "- Both XGBoost and Random Forest are **significantly better** than Logistic Regression.\n",
        "- **No statistically significant difference** in overall cross-validated F1 scores between XGBoost and Random Forest—either can be justified for deployment based on these metrics.\n",
        "- **However, McNemar’s test** reveals that the pattern of errors made by XGBoost and Random Forest is **statistically different**—they do not make mistakes on the same samples. This could be leveraged for model ensembling or for selecting a model based on specific business needs.\n",
        "- **Welch ANOVA and Games-Howell** reinforce that the biggest leap in performance is from linear to ensemble models, not between the ensembles themselves.\n",
        "\n",
        "---\n",
        "\n",
        "**In conclusion:**  \n",
        "All advanced statistical tests confirm that ensemble methods (XGBoost, Random Forest) are vastly superior to Logistic Regression for this problem, but differences between the two ensembles are small in practical terms. Model choice between XGBoost and Random Forest can therefore be based on interpretability, speed, or integration needs rather than pure performance.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfzPaE11RStm"
      },
      "source": [
        "# Why Macro F1 and ROC AUC Are Preferred Here\n",
        "Macro F1 considers both classes equally, making it the most robust metric for imbalanced datasets—where both missing defaulters and misclassifying good customers matter.\n",
        "\n",
        "ROC AUC evaluates model discrimination across all thresholds, providing an overall sense of model skill regardless of the decision cutoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjYstRKPRStm"
      },
      "outputs": [],
      "source": [
        "\n",
        "diff = np.array(cv_scores[\"XGBoost\"]) - np.array(cv_scores[\"RandomForest\"])\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(diff, marker='o', linestyle='-', color='teal')\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Fold-wise Macro F1 Differences (XGBoost - Random Forest)\")\n",
        "plt.xlabel(\"CV Fold\")\n",
        "plt.ylabel(\"F1 Difference\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "FMHs5QJ35cs8"
      },
      "source": [
        "### Observations Based on Fold-wise Macro F1 Scores ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWQcQ81mRStm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# McNemar contingency table (from your output)\n",
        "tbl = np.array([[1584, 66],\n",
        "                [30, 108]])\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(tbl, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[\"RF Correct/XGB Wrong\", \"Both Correct\"],\n",
        "            yticklabels=[\"Both Wrong\", \"RF Wrong/XGB Correct\"])\n",
        "plt.title(\"McNemar's Test Contingency Table\\n(XGBoost vs Random Forest)\")\n",
        "plt.ylabel(\"True Outcome\")\n",
        "plt.xlabel(\"Prediction Outcome\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "LIUynfPF5cs8"
      },
      "source": [
        "### Observations Based on the McNemar Contingency Analysis\n",
        "\n",
        "- The contingency table shows a high level of agreement between the two models, with a large number of cases where both models either correctly or incorrectly classified the same samples.\n",
        "- There were 28 instances where one model (e.g., XGBoost) was correct while the other (e.g., Random Forest) misclassified, compared to 19 instances in the opposite direction.\n",
        "- The relatively small number of discordant pairs, in comparison to the total number of cases, indicates that the error patterns of the two models are quite similar.\n",
        "- The McNemar test yielded a p-value (≈0.24) that is above the common significance threshold (0.05), suggesting that the differences in their misclassification patterns are not statistically significant.\n",
        "- Overall, the results imply that both models make largely consistent predictions on the test set, which could be advantageous when considering model ensembling or choosing one model based on other factors such as interpretability or computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNRyaBgQRStn"
      },
      "outputs": [],
      "source": [
        "posthoc_labels = [\n",
        "    \"LogReg vs RandomForest\",\n",
        "    \"LogReg vs XGBoost\",\n",
        "    \"RandomForest vs XGBoost\"\n",
        "]\n",
        "posthoc_diffs = [-0.176, -0.181, -0.005]\n",
        "posthoc_errors = [0.013, 0.013, 0.011]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.barh(posthoc_labels, posthoc_diffs, xerr=posthoc_errors, color='skyblue')\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title(\"Games-Howell Pairwise Mean Macro F1 Differences\")\n",
        "plt.xlabel(\"Mean F1 Difference (A - B)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "l1F4bb5j5cs8"
      },
      "source": [
        "### Observations Based on the Games-Howell Pairwise Mean Macro F1 Differences\n",
        "\n",
        "- **Logistic Regression vs. Ensemble Models:**  \n",
        "    Logistic Regression shows a significant drop in macro F1 performance compared to both Random Forest and XGBoost. The mean differences of approximately –0.174 and –0.178 (LogReg vs. Random Forest and LogReg vs. XGBoost, respectively) with extremely low p-values indicate that Logistic Regression consistently underperforms in capturing balanced performance between defaulters and non‑defaulters.\n",
        "\n",
        "- **Ensemble Methods Comparison:**  \n",
        "    The mean difference between Random Forest and XGBoost is very small (around –0.00367) with a very high p-value (approximately 0.90), suggesting no statistically significant difference between these two models. Their macro F1 scores are essentially equivalent.\n",
        "\n",
        "- **Overall Implication:**  \n",
        "    The post-hoc Games-Howell analysis reinforces that ensemble methods (Random Forest and XGBoost) are superior in terms of macro F1 score compared to the linear approach of Logistic Regression, while there’s no meaningful performance gap between the two ensemble models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxi3dF0ZRStn"
      },
      "source": [
        "# In the context of loan default prediction, the most critical performance measures are:\n",
        "\n",
        "**Macro F1 Score**– to ensure balanced performance across both borrower types,\n",
        "\n",
        "**Recall** – to minimize undetected defaulters, which are most costly to the bank,\n",
        "\n",
        "**ROC AUC** – to evaluate overall discriminative power of the model.\n",
        "\n",
        "Metrics like accuracy and precision are included for completeness but are not sufficient for reliable model selection in imbalanced, risk-sensitive tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7souO0cRStn",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "# Model Comparison\n",
        "---\n",
        "\n",
        "## Performance Metrics Overview\n",
        "\n",
        "| Model               | Accuracy  | Precision  | Recall     | F1 Score   | ROC AUC   |\n",
        "|---------------------|-----------|------------|------------|------------|-----------|\n",
        "| **XGBoost**         | 0.928     | 0.821      | 0.821      | 0.821      | 0.961     |\n",
        "| **Random Forest**   | 0.908     | 0.777      | 0.754      | 0.765      | 0.952     |\n",
        "| **Logistic Regression** | 0.766 | 0.439      | 0.619      | 0.514      | 0.781     |\n",
        "\n",
        "- **Best f1_macro (CV):**\n",
        "    - XGBoost: **0.8776**\n",
        "    - RandomForest: **0.8610**\n",
        "    - LogisticRegression: **0.6850**\n",
        "\n",
        "- **Best Hyperparameters:**  \n",
        "    - *XGBoost*: colsample_bytree=1.0, learning_rate=0.3, max_depth=6, n_estimators=200, reg_alpha=0.5, reg_lambda=1, subsample=0.8  \n",
        "    - *RandomForest*: bootstrap=False, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200  \n",
        "    - *LogisticRegression*: C=1, max_iter=500, penalty=l2, solver=liblinear\n",
        "\n",
        "---\n",
        "\n",
        "## Observations and Interpretation\n",
        "\n",
        "### 1. **Model Discrimination and Generalization**\n",
        "- **XGBoost** outperforms the other models across all metrics. With an F1 score of 0.82 and ROC AUC of 0.96, it demonstrates both high accuracy and robust class separation, which is essential for predicting defaults (imbalanced classification). This is reinforced by the highest CV f1_macro.\n",
        "- **Random Forest** is a strong baseline, also showing excellent ROC AUC (0.95), but lags slightly behind XGBoost in F1 and recall, suggesting marginally weaker detection of defaulters.\n",
        "- **Logistic Regression** underperforms significantly, with an F1 score barely above 0.5 and much lower precision. This suggests that the linear model is insufficiently expressive for the complexity of the relationships in the data.\n",
        "\n",
        "### 2. **Business Impact**\n",
        "- **False Negatives** (predicting non-default when a default occurs) are especially costly for the bank. Both XGBoost and Random Forest offer a much better balance between precision and recall compared to Logistic Regression, reducing this business risk.\n",
        "- **Interpretability** is crucial per project requirements. While XGBoost and Random Forest are less transparent than Logistic Regression, feature importance techniques (e.g., SHAP, permutation importance) can provide necessary justification for decisions, as required by banking regulation.\n",
        "\n",
        "### 3. **Hyperparameter Tuning**\n",
        "- Substantial gains are realized via hyperparameter tuning, especially with XGBoost (notably, high values for n_estimators and a moderate learning rate).\n",
        "- Both ensemble models favor deeper trees and more estimators, suggesting complex interactions in the data.\n",
        "\n",
        "### 4. **Recommendations for Final Solution**\n",
        "- **XGBoost** is recommended as the primary model for deployment due to superior predictive performance. Interpretability tools must be integrated to meet regulatory requirements.\n",
        "- Further marginal improvements may be possible via ensemble stacking or additional feature engineering.\n",
        "- Logistic Regression, though interpretable, is not suitable as a primary model given current results. However, its coefficients can be used for supplementary insights into linear feature relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion and Next Steps\n",
        "\n",
        "- **XGBoost** provides the best trade-off between predictive power and business requirements. Its adoption would help the bank minimize losses due to default, while feature attribution methods can maintain model transparency.\n",
        "- For production, monitoring of model drift, regular retraining, and integration with interpretability frameworks are strongly recommended.\n",
        "- Continued evaluation using business-focused metrics (e.g., cost of misclassification, profit curves) may be advisable to align model performance with bank objectives.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV8M8V4_RSto"
      },
      "source": [
        "## Evaluation of the Best Model: ROC-AUC and Confusion Matrix\n",
        "\n",
        "Below, we evaluate the best-performing model (as selected by cross-validated score) on the holdout test set. We report the ROC-AUC value and present both the ROC curve and the confusion matrix to illustrate the model's discriminatory power and error distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Hjo7RoRSto"
      },
      "outputs": [],
      "source": [
        "# Predict probabilities and classes\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC of the best model ({best_model_name}): {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve and Confusion Matrix side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "axes[0].plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "axes[0].set_xlabel(\"False Positive Rate\")\n",
        "axes[0].set_ylabel(\"True Positive Rate\")\n",
        "axes[0].set_title(f\"ROC Curve - {best_model_name}\")\n",
        "axes[0].legend(loc=\"lower right\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=axes[1], cmap=\"Blues\", colorbar=False)\n",
        "axes[1].set_title(f\"Confusion Matrix - {best_model_name}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report for completeness\n",
        "print(f\"\\nClassification report for the best model ({best_model_name}):\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r--TkfSzRSto"
      },
      "source": [
        "## Saving the Best Model and Interpreting Predictions with SHAP\n",
        "\n",
        "After hyperparameter tuning and evaluation, the best-performing model is identified and saved for future deployment.  \n",
        "To ensure regulatory compliance and business transparency, SHAP (SHapley Additive exPlanations) analysis is conducted to interpret the feature impact on the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX7Y3m4cRStp"
      },
      "source": [
        "Model Saving:\n",
        "Saves the entire pipeline (preprocessing + model). This guarantees you can load and use the model on raw data in production.\n",
        "\n",
        "SHAP:\n",
        "Uses the trained classifier on the correctly preprocessed data, ensuring explanations match the actual prediction process.\n",
        "\n",
        "Feature Names:\n",
        "Handles both numeric and one-hot-encoded categorical features, improving interpretability of the summary plot.\n",
        "\n",
        "Visualization:\n",
        "The summary plot gives a global view of which features drive predictions (for compliance and business explanations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MVl_mFaRStp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Find the best model according to validation scores\n",
        "best_model_name = max(mgr.best_scores_, key=mgr.best_scores_.get)\n",
        "best_pipeline = mgr.best_estimators_[best_model_name]\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "\n",
        "# 2. Save the full pipeline (preprocessing + model) for deployment\n",
        "joblib.dump(best_pipeline, 'best_model.pkl')\n",
        "\n",
        "# To load later: loaded_pipeline = joblib.load('best_model.pkl')\n",
        "\n",
        "# 3. Prepare data for SHAP analysis\n",
        "# Extract the classifier and the fitted preprocessor\n",
        "clf = best_pipeline.named_steps['clf']\n",
        "preprocessor = best_pipeline.named_steps['preprocessor']\n",
        "\n",
        "# Transform train and test data to match model input\n",
        "X_train_shap = preprocessor.transform(X_train)\n",
        "X_test_shap = preprocessor.transform(X_test)\n",
        "\n",
        "# 4. Build the SHAP explainer\n",
        "# For tree-based models, SHAP will use TreeExplainer automatically\n",
        "explainer = shap.Explainer(clf, X_train_shap)\n",
        "\n",
        "# 5. Compute SHAP values for the test set\n",
        "shap_values = explainer(X_test_shap)\n",
        "\n",
        "# 6. Visualize feature importance (SHAP summary plot)\n",
        "# Use original feature names for interpretability\n",
        "try:\n",
        "    num_feature_names = preprocessor.named_transformers_['num'].get_feature_names_out(num_cols).tolist()\n",
        "    cat_feature_names = []\n",
        "    if cat_cols:  # only try to extract names if there are categorical columns\n",
        "        cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(cat_cols).tolist()\n",
        "    feature_names = num_feature_names + cat_feature_names\n",
        "except Exception:\n",
        "    # Fallback: Just use column names if get_feature_names_out is unavailable\n",
        "    feature_names = num_cols + cat_cols\n",
        "\n",
        "shap.summary_plot(shap_values, X_test_shap, feature_names=feature_names)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqwfYesFRStp"
      },
      "source": [
        "### SHAP Summary Plot – Key Observations\n",
        "\n",
        "- **DEBTINC (Debt-to-Income Ratio) is the most influential feature** in the model. High debt-to-income values strongly increase the predicted risk of loan default, as indicated by the concentration of red points with large positive SHAP values.\n",
        "\n",
        "- **CLAGE (Age of Oldest Credit Line) also plays a significant role**. The model indicates that applicants with a longer credit history (higher CLAGE) have a higher risk of default in this dataset, which may be a reflection of specific risk profiles in the population.\n",
        "\n",
        "- **DELINQ (Number of Delinquencies) has a strong positive effect** on default risk. Applicants with more previous delinquencies are much more likely to default, as shown by the rightward shift of red dots.\n",
        "\n",
        "- **Features such as YOJ (Years at Current Job), CLNO (Number of Credit Lines), LOAN, VALUE, and MORTDUE** contribute to predictions but have a smaller overall impact compared to the top features. Their effects are more nuanced and less pronounced.\n",
        "\n",
        "- **DEROG (Major Derogatory Reports) and NINQ (Number of Recent Inquiries)**, while included in the model, show relatively limited influence on the model’s output compared to the other features.\n",
        "\n",
        "- **General Pattern:**  \n",
        "  - Features where high values (red) appear on the right increase the risk of default when their value is high.\n",
        "  - Features where low values (blue) appear on the left decrease the risk of default.\n",
        "\n",
        "- **Model Interpretability:**  \n",
        "  - The SHAP plot provides transparency, allowing stakeholders to understand which variables drive the model's decisions.\n",
        "  - These insights support regulatory compliance and help prioritize risk factors for further business investigation.\n",
        "\n",
        "**Conclusion:**  \n",
        "The SHAP analysis confirms that the model's predictions are most strongly driven by the applicant’s debt-to-income ratio, age of credit history, and prior delinquencies. These features should be closely monitored in both model development and business decision-making.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOiP35XH5cs_"
      },
      "outputs": [],
      "source": [
        "print(feature_names)\n",
        "print(len(feature_names), X_test_shap.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1oVKEmF5cs_"
      },
      "outputs": [],
      "source": [
        "X_test_df  = pd.DataFrame(X_test_shap,  columns=feature_names)\n",
        "shap_values = explainer(X_test_df)\n",
        "shap.plots.bar(shap_values, max_display=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "W0Wy14qL5cs_"
      },
      "source": [
        "### Observations from the SHAP Bar Plot\n",
        "\n",
        "- **DEBTINC Dominance:**  \n",
        "    The Debt-to-Income Ratio (DEBTINC) stands out with the highest average absolute SHAP value, indicating it is the most influential feature driving the model’s predictions. Higher values of DEBTINC are strongly associated with an increased risk of loan default.\n",
        "\n",
        "- **Importance of DELINQ and CLAGE:**  \n",
        "    The number of Delinquent Credit Lines (DELINQ) and the age of the oldest Credit Line (CLAGE) also emerge as key contributors. Their relatively high SHAP values suggest that more past delinquencies and longer credit histories significantly impact the prediction towards default.\n",
        "\n",
        "- **Moderate Contribution of Other Features:**  \n",
        "    Other features (such as MORTDUE and DEROG) contribute to the model’s output, but their impact is notably lower compared to DEBTINC, DELINQ, and CLAGE.\n",
        "\n",
        "- **Overall Insight:**  \n",
        "    The SHAP analysis confirms that the model largely bases its decisions on financial stability indicators, particularly the borrower’s debt load and credit behavior. This insight is critical for understanding the risk factors driving loan default predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-t1y_W7RStq"
      },
      "source": [
        "# Fit XGBoost classifier with best hyperparameters to most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtpTYHTnRStr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Define selected SHAP features\n",
        "selected_features = ['DEBTINC', 'CLAGE', 'DELINQ']\n",
        "\n",
        "# 2. Prepare data with only these features from the original DataFrame\n",
        "X_train_sel = X_train[selected_features]\n",
        "X_test_proc_sel = X_test[selected_features]\n",
        "\n",
        "# 3. Identify column types for proper preprocessing from the DataFrame\n",
        "cat_cols = X_train_sel.select_dtypes(include='object').columns.tolist()\n",
        "num_cols = X_train_sel.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# 4. Create preprocessing transformer for selected features\n",
        "preprocessor_shap = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)\n",
        "])\n",
        "\n",
        "# 5. Set class weight for imbalance (as in previous pipeline)\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# 6. Create the full pipeline: preprocessing + SMOTE + classifier\n",
        "pipeline_shap = ImbPipeline([\n",
        "    ('preprocessor', preprocessor_shap),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', XGBClassifier(\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        tree_method='hist'\n",
        "    ))\n",
        "])\n",
        "# 7. Fit the pipeline and evaluate on test data using the DataFrame with selected features\n",
        "pipeline_shap.fit(X_train_sel, y_train)\n",
        "y_pred_shap = pipeline_shap.predict(X_test_proc_sel)\n",
        "y_proba_shap = pipeline_shap.predict_proba(X_test_proc_sel)[:, 1]\n",
        "\n",
        "print(selected_features)\n",
        "print(classification_report(y_test, y_pred_shap))\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"F1-score:  {f1_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_shap):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRDSS5e-RSts"
      },
      "outputs": [],
      "source": [
        "mean_abs_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "feature_ranking = np.argsort(mean_abs_shap)[::-1]\n",
        "\n",
        "# Teraz feature_names[feature_ranking[:N]] to NAJWAŻNIEJSZE cechy z punktu widzenia modelu\n",
        "for N in range(3, 6):\n",
        "    top_features = [feature_names[i] for i in feature_ranking[:N]]\n",
        "    print(f\"Top {N} SHAP features (po transformacji): {top_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OouNO1PA5ctA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Define selected SHAP features\n",
        "selected_features = ['DEBTINC', 'DELINQ', 'CLAGE', 'MORTDUE']\n",
        "\n",
        "# 2. Prepare data with only these features from the original DataFrame\n",
        "X_train_sel = X_train[selected_features]\n",
        "X_test_proc_sel = X_test[selected_features]\n",
        "\n",
        "# 3. Identify column types for proper preprocessing from the DataFrame\n",
        "cat_cols = X_train_sel.select_dtypes(include='object').columns.tolist()\n",
        "num_cols = X_train_sel.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# 4. Create preprocessing transformer for selected features\n",
        "preprocessor_shap = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)\n",
        "])\n",
        "\n",
        "# 5. Set class weight for imbalance (as in previous pipeline)\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# 6. Create the full pipeline: preprocessing + SMOTE + classifier\n",
        "pipeline_shap = ImbPipeline([\n",
        "    ('preprocessor', preprocessor_shap),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', XGBClassifier(\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        tree_method='hist'\n",
        "    ))\n",
        "])\n",
        "# 7. Fit the pipeline and evaluate on test data using the DataFrame with selected features\n",
        "pipeline_shap.fit(X_train_sel, y_train)\n",
        "y_pred_shap = pipeline_shap.predict(X_test_proc_sel)\n",
        "y_proba_shap = pipeline_shap.predict_proba(X_test_proc_sel)[:, 1]\n",
        "\n",
        "print(selected_features)\n",
        "print(classification_report(y_test, y_pred_shap))\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"F1-score:  {f1_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_shap):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkjYuztR5ctB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Define selected SHAP features\n",
        "selected_features = ['DEBTINC', 'DELINQ', 'CLAGE', 'MORTDUE', 'DEROG']\n",
        "\n",
        "# 2. Prepare data with only these features from the original DataFrame\n",
        "X_train_sel = X_train[selected_features]\n",
        "X_test_proc_sel = X_test[selected_features]\n",
        "\n",
        "# 3. Identify column types for proper preprocessing from the DataFrame\n",
        "cat_cols = X_train_sel.select_dtypes(include='object').columns.tolist()\n",
        "num_cols = X_train_sel.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# 4. Create preprocessing transformer for selected features\n",
        "preprocessor_shap = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)\n",
        "])\n",
        "\n",
        "# 5. Set class weight for imbalance (as in previous pipeline)\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# 6. Create the full pipeline: preprocessing + SMOTE + classifier\n",
        "pipeline_shap = ImbPipeline([\n",
        "    ('preprocessor', preprocessor_shap),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', XGBClassifier(\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        tree_method='hist'\n",
        "    ))\n",
        "])\n",
        "# 7. Fit the pipeline and evaluate on test data using the DataFrame with selected features\n",
        "pipeline_shap.fit(X_train_sel, y_train)\n",
        "y_pred_shap = pipeline_shap.predict(X_test_proc_sel)\n",
        "y_proba_shap = pipeline_shap.predict_proba(X_test_proc_sel)[:, 1]\n",
        "\n",
        "print(selected_features)\n",
        "print(classification_report(y_test, y_pred_shap))\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"F1-score:  {f1_score(y_test, y_pred_shap):.4f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_shap):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-TUGEJf5ctB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a new pipeline that uses only the most important features from the best model.\n",
        "# Here, we select the top 3 features: 'DEBTINC', 'DELINQ', and 'CLAGE'\n",
        "\n",
        "\n",
        "selected_features = ['DEBTINC', 'DELINQ', 'CLAGE']\n",
        "\n",
        "# Create a preprocessor that scales only the selected features\n",
        "preprocessor_sel = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), selected_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Retrieve the best model's classifier from the manager pipeline\n",
        "# (The best pipeline already contains a preprocessor and SMOTE, but here we rebuild the pipeline to work on only the selected features.)\n",
        "best_clf = mgr.best_estimators_[best_model_name].named_steps['clf']\n",
        "\n",
        "# Build a new pipeline using the selected features\n",
        "pipeline_sel = ImbPipeline([\n",
        "    ('preprocessor', preprocessor_sel),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('clf', best_clf)\n",
        "])\n",
        "\n",
        "# Fit the new pipeline on training data using only the selected features\n",
        "pipeline_sel.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# Evaluate on the test set (using the same selected features)\n",
        "y_pred_sel = pipeline_sel.predict(X_test[selected_features])\n",
        "y_proba_sel = pipeline_sel.predict_proba(X_test[selected_features])[:, 1]\n",
        "\n",
        "print(\"Performance of the best model on selected features:\")\n",
        "print(classification_report(y_test, y_pred_sel))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_sel))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfd-bVvRRStt",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### Observations on the Classification Report and Selected Features\n",
        "\n",
        "- **Class-Specific Performance:**\n",
        "    - **Non-Default Class (0):**\n",
        "        - **Precision:** 0.95 indicates that when the model predicts a non-default, it is highly accurate.\n",
        "        - **Recall:** 0.89 shows that most non-default cases are correctly identified.\n",
        "        - **F1-Score:** 0.92 confirms strong overall performance for class 0.\n",
        "    - **Default Class (1):**\n",
        "        - **Precision:** 0.64 suggests that a notable proportion of non-defaulter predictions are incorrect when predicting a default.\n",
        "        - **Recall:** 0.82 implies that the model is effective at capturing the majority of actual defaulters.\n",
        "        - **F1-Score:** 0.72 represents a reasonable balance between precision and recall for this minority class.\n",
        "\n",
        "- **Overall Metrics:**\n",
        "    - **Accuracy:** 87% accuracy on the test set.\n",
        "    - **Macro Average:** A precision of 0.80, recall of 0.85, and F1-score of 0.82, reflecting balanced performance across both classes.\n",
        "    - **Weighted Average:** Metrics (precision = 0.89, recall = 0.87, F1-score = 0.88) are influenced by the larger non-default class and support overall model robustness.\n",
        "\n",
        "- **Insights from Feature Selection and Pipeline:**\n",
        "    - The pipeline was re-trained using a subset of top features (_e.g._, `DEBTINC`, `CLAGE`, `DELINQ`, `YOJ`, `CLNO`) after SHAP-based selection.\n",
        "    - The choice of a limited yet influential feature set can simplify the model while retaining competitive predictive performance.\n",
        "    - The use of SMOTE in the pipeline helps mitigate class imbalance and contributes to the higher recall for the default class.\n",
        "\n",
        "- **Business Implications:**\n",
        "    - The high recall for the default class (0.82) is critical in risk-sensitive applications like loan default prediction, as it minimizes missed high-risk applicants.\n",
        "    - However, lower precision for the default class (0.64) indicates some false alarms, which might lead to some good applicants being flagged. Further calibration of decision thresholds could help balance this trade-off.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLZFgwldRStt"
      },
      "source": [
        "# Final Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb2uVwEBRStt"
      },
      "source": [
        "**Key ROC Curve Observations:**\n",
        "\n",
        "1. **Ensemble Models Performance:**  \n",
        "    The ROC curves for the ensemble models (Random Forest and XGBoost) quickly rise towards a high true positive rate while keeping the false positive rate low. This indicates that at lower FPR thresholds, both models capture a high percentage of defaulters.\n",
        "\n",
        "2. **High ROC-AUC Scores:**  \n",
        "    The Random Forest model achieves an ROC-AUC of around 0.96, and the XGBoost model has an ROC-AUC of approximately 0.96. These high values demonstrate excellent discrimination ability between defaulters and non‑defaulters, especially when compared to simpler baseline models like Logistic Regression.\n",
        "\n",
        "3. **Sensitivity at Low False Positive Rates:**  \n",
        "    The steep initial portion of the ROC curves signifies that even when the false positive rate is minimal, the models are still able to detect a significant portion of positives (defaulters). This sensitivity is crucial in risk-sensitive applications, where failing to detect a defaulter can be costly.\n",
        "\n",
        "4. **Advantage of Ensemble Methods:**  \n",
        "    The overall shape of the ROC curves confirms that ensemble methods, by capturing non-linear patterns in the data, are a better fit for this dataset relative to simpler models. They significantly reduce the number of false negatives.\n",
        "\n",
        "**Summary:**  \n",
        "- Ensemble models (Random Forest and XGBoost) yield ROC-AUC scores of approximately 0.96 and 0.96, respectively.  \n",
        "- The steep climb observed in the ROC curves demonstrates high sensitivity with a low rate of false positives.  \n",
        "- Together, these observations validate that ensemble approaches are highly effective at distinguishing between defaulters and non‑defaulters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI5w9Ww0RStu",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "### Observations from the Confusion Matrix Comparisons\n",
        "\n",
        "- **Logistic Regression:**  \n",
        "    - The confusion matrix shows that almost all test instances are classified as the non-default class.\n",
        "    - This heavy bias toward the negative class indicates a very high number of false negatives (i.e. defaulters being missed).\n",
        "    - High specificity comes at the cost of very low recall, making the model unsuitable for detecting defaulters.\n",
        "\n",
        "- **Tuned Random Forest:**  \n",
        "    - The confusion matrix for the tuned Random Forest exhibits a better balance by correctly identifying a higher number of defaulters.\n",
        "    - Fewer false negatives are observed compared to Logistic Regression, indicating improved sensitivity.\n",
        "    - This model seems to balance precision and recall more effectively—important for risk-sensitive applications.\n",
        "\n",
        "- **Tuned XGBoost:**  \n",
        "    - Similar to Random Forest, the confusion matrix for XGBoost shows a low number of false negatives.\n",
        "    - The overall distribution of true positives and true negatives is competitive with the Random Forest, with only marginal differences.\n",
        "    - Despite slight variations, XGBoost still captures the minority class better than Logistic Regression.\n",
        "\n",
        "**Summary:**  \n",
        "Ensemble methods (Random Forest and XGBoost) clearly outperform Logistic Regression by reducing the number of false negatives. This makes them more reliable for loan default prediction, as they are more effective at identifying high-risk applicants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4v3rxabRStu",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "**Refined Insights:**\n",
        "- Default risk is strongly influenced by delinquency counts (DELINQ) and credit history measures (CLAGE, DEROG), suggesting that previous credit trouble is a key indictor of future defaults.\n",
        "- Loan and mortgage amounts (LOAN, MORTDUE) show wide ranges, hinting that segmentation or transformation might further improve risk evaluation.\n",
        "- Different loan purposes (REASON) exhibit distinct default rates, with 'HomeImp' potentially carrying a slightly higher risk than 'DebtCon'.\n",
        "- Ensemble models (Random Forest and XGBoost) clearly outperform Logistic Regression, demonstrating that complex interactions in the data are best captured using these methods.\n",
        "- SHAP analyses confirm the dominance of features like DELINQ, CLAGE, and DEBTINC, reinforcing their importance in informing both risk models and business decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA5UjwizRStu",
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "\n",
        "\n",
        "## Recommendations\n",
        "1. **Deep Dive on Influential Features:**  \n",
        "    Explore further the features with high SHAP absolute values (e.g., DELINQ, CLAGE, DEBTINC) to derive domain-specific insights that could refine risk evaluation strategies.\n",
        "2. **Model Decision Thresholds:**  \n",
        "    Given the strong ROC-AUC and Precision-Recall metrics, consider adjusting classification thresholds to optimize false negative rates, particularly considering the cost associated with misclassifying defaulters.\n",
        "3. **Communication and Transparency:**  \n",
        "    Use the SHAP summary and feature importance plots to clearly convey how different features contribute to predictions in stakeholder communications.\n",
        "4. **Further Exploration of Subsets:**  \n",
        "    Validate the stability of SHAP value distributions by examining different sample subsets or through cross-validation, ensuring that the model’s interpretability holds across various scenarios.\n",
        "5. **Continuous Monitoring and Refinement:**  \n",
        "    Compare the performance of the current tuned ensemble models (Random Forest and XGBoost) with the baseline logistic regression to monitor performance over time and identify any need for refinement in preprocessing or model tuning.\n",
        "\n",
        "These observations and recommendations support a data-driven approach in enhancing model understanding and risk management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0KAZCBxRStu"
      },
      "source": [
        "\n",
        "## **Executive Summary**\n",
        "\n",
        "The loan default risk modeling project addressed the critical need to identify high-risk borrowers using data-driven machine learning techniques.\n",
        "\n",
        "**Key findings from Milestone 1:**\n",
        "- The dataset (HMEQ) was imbalanced, with default (BAD=1) being the minority class.\n",
        "- Data preprocessing included outlier treatment, missing value imputation, encoding categorical variables, and scaling numerics.\n",
        "- Exploratory analysis revealed that variables such as debt-to-income ratio (`DEBTINC`), number of delinquent lines (`DELINQ`), and age of oldest credit line (`CLAGE`) are strong predictors of default.\n",
        "\n",
        "**Final proposed model:**\n",
        "- The best-performing model is an XGBoost classifier, tuned using GridSearchCV and balanced with SMOTE and class weighting.\n",
        "- After SHAP-based feature importance analysis, a simplified model using the top 4-5 most important features achieved almost identical performance to the full-feature model, enabling easier regulatory compliance and business transparency.\n",
        "\n",
        "**Next steps:**\n",
        "- Deploy the final XGBoost pipeline in the loan approval process.\n",
        "- Integrate SHAP explanations for every decision to support explainability and compliance.\n",
        "- Monitor model performance over time and periodically retrain on new data.\n",
        "- Continue analyzing customer segments and edge cases to further reduce risk and optimize thresholds.\n",
        "\n",
        "---\n",
        "\n",
        "## **Problem and Solution Summary**\n",
        "\n",
        "**Problem summary:**\n",
        "- The business challenge is to **reduce financial losses** from loan defaults while maintaining high approval rates for good customers.\n",
        "- Predicting defaults accurately is complicated by the imbalanced nature of historical defaults and the need for transparent, auditable decisions.\n",
        "\n",
        "**Proposed solution design:**\n",
        "- The solution is an end-to-end machine learning pipeline using XGBoost, enhanced by SMOTE balancing and thorough feature engineering.\n",
        "- SHAP-based explainability is incorporated to make every risk prediction transparent for auditors and decision-makers.\n",
        "- The final model can operate with as few as four key variables (`DEBTINC`, `CLAGE`, `DELINQ`, `LOAN`), without significant loss of predictive power.\n",
        "\n",
        "**Business impact:**\n",
        "- Enables the bank to identify high-risk applications more accurately, reducing expected losses.\n",
        "- Increases trust and transparency for both regulators and internal stakeholders.\n",
        "- Streamlines approval processes for low-risk customers, supporting business growth.\n",
        "\n",
        "---\n",
        "\n",
        "## **Recommendations for Implementation**\n",
        "\n",
        "**Key recommendations:**\n",
        "- **Productionize** the final XGBoost model as part of the automated loan approval workflow.\n",
        "- Use SHAP explanations to provide individualized reason codes for loan decisions.\n",
        "- Set up regular performance monitoring and schedule retraining every 3–6 months.\n",
        "- Ensure data pipelines remain robust, with alerts for data drift or quality issues.\n",
        "\n",
        "**Key actionables for stakeholders:**\n",
        "- Credit/risk team: Review and approve the simplified scoring model.\n",
        "- IT: Integrate the Python pipeline and SHAP reporting into existing systems.\n",
        "- Compliance: Audit model explanations and monitor for bias or unfair impact.\n",
        "\n",
        "**Expected benefits:**\n",
        "- Reduction in default-related financial losses (potentially 10–20% annual savings based on historic portfolio).\n",
        "- Improved efficiency and turnaround time for loan applications.\n",
        "- Enhanced regulatory compliance, reducing audit risk and potential fines.\n",
        "\n",
        "**Estimated costs and numbers (example rational assumption):**\n",
        "- Implementation and integration cost: ~$50k (model development, IT integration, training).\n",
        "- Expected annual benefit: For a $10M portfolio, a 1% reduction in default rate saves $100,000 per year.\n",
        "\n",
        "**Key risks and challenges:**\n",
        "- **Model drift** if customer behavior changes — mitigated by monitoring and retraining.\n",
        "- **Data quality issues** (e.g., missing or delayed variables) could impact predictions.\n",
        "- **Regulatory changes** may require updating the feature set or model logic.\n",
        "\n",
        "**Further analysis/future work:**\n",
        "- Deepen analysis of high-risk subsegments for tailored interventions.\n",
        "- Explore alternative algorithms (e.g., LightGBM, explainable boosting machines) for continuous improvement.\n",
        "- Consider deployment of dual models (full and minimal feature set) for different approval scenarios.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ik5_PONRStv"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "from nbconvert import HTMLExporter\n",
        "\n",
        "# Define the notebook file name\n",
        "notebook_filename = \"Applied_Data_Science_Loan_Default_Zuzanna_Walus.ipynb\"\n",
        "\n",
        "# Load the notebook content\n",
        "with open(notebook_filename, 'r', encoding='utf-8') as f:\n",
        "    notebook_content = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Convert the notebook to HTML\n",
        "html_exporter = HTMLExporter()\n",
        "html_data, _ = html_exporter.from_notebook_node(notebook_content)\n",
        "\n",
        "# Save the HTML file\n",
        "output_filename = \"Applied_Data_Science_Loan_Default_Zuzanna_Walus.ipynb.html\"\n",
        "with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_data)\n",
        "\n",
        "print(f\"Notebook successfully exported to {output_filename}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}